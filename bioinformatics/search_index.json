[["index.html", "The Earth Hologenome Initiative Bioinformatics Workflow Chapter 1 Introduction 1.1 Workflow overview 1.2 Snakemake pipeline", " The Earth Hologenome Initiative Bioinformatics Workflow Raphael Eisenhofer1 Antton Alberdi2 2023-11-06 Chapter 1 Introduction The Earth Hologenome Initiative (EHI, www.earthhologenome.org) is a global collaborative endeavour aimed at promoting, facilitating, coordinating, and standardising hologenomic research on wild organisms worldwide. The EHI encompasses projects with diverse study designs and goals around standardised and open access sample collection and preservation, data generation and data management criteria. One of the main objectives of the EHI is to facilitate analysis of animal genomic and microbial metagenomic data. Here, you will find a summary of the bioinformatic pipeline we use for generating analytical data from raw sequencing files. 1.1 Workflow overview Description of pipeline 1.2 Snakemake pipeline Description of Snakemake University of Copenhagen, raphael.eisenhofer@sund.ku.dk↩︎ University of Copenhagen, antton.alberdi@sund.ku.dk↩︎ "],["data-preprocessing.html", "Chapter 2 Data preprocessing 2.1 Quality-filtering 2.2 Host mapping 2.3 Metagenomic complexity assessment 2.4 Prokaryotic fraction assessment", " Chapter 2 Data preprocessing Contents here. 2.1 Quality-filtering fastp \\ --in1 {input.r1i} --in2 {input.r2i} \\ --out1 {output.r1o} --out2 {output.r2o} \\ --trim_poly_g \\ --trim_poly_x \\ --low_complexity_filter \\ --n_base_limit 5 \\ --qualified_quality_phred 20 \\ --length_required 60 \\ --thread {threads} \\ --html {output.fastp_html} \\ --json {output.fastp_json} \\ --adapter_sequence {params.adapter1} \\ --adapter_sequence_r2 {params.adapter2} 2.2 Host mapping # Map reads to catted reference using Bowtie2 bowtie2 \\ --time \\ --threads {threads} \\ -x {input.catted_ref} \\ -1 {input.r1i} \\ -2 {input.r2i} \\ | samtools view -b -@ {threads} - | samtools sort -@ {threads} -o {output.all_bam} - &amp;&amp; # Extract non-host reads (note we&#39;re not compressing for nonpareil) samtools view -b -f12 -@ {threads} {output.all_bam} \\ | samtools fastq -@ {threads} -1 {output.non_host_r1} -2 {output.non_host_r2} - &amp;&amp; # Send host reads to BAM samtools view -b -F12 -@ {threads} {output.all_bam} \\ | samtools sort -@ {threads} -o {output.host_bam} - # Get % duplicates from host BAM picard -Xmx{resources.mem_gb}g MarkDuplicates \\ -I {output.host_bam} \\ -O {output.markdup_bam} \\ -M {output.markdup} 2.3 Metagenomic complexity assessment nonpareil \\ -s {input.non_host_r1} \\ -f fastq \\ -T kmer \\ -t {threads} \\ -b {wildcards.sample} #Script to extract nonpareil values of interest Rscript {config[codedir]}/scripts/nonpareil_table.R {output.npo} {output.npstats} 2.4 Prokaryotic fraction assessment #Run singlem pipe singlem pipe \\ -1 {input.non_host_r1} \\ -2 {input.non_host_r2} \\ --otu-table {params.pipe_uncompressed} \\ --taxonomic-profile {output.condense} \\ --threads {threads} #Run singlem read_fraction singlem read_fraction \\ -1 {input.non_host_r1} \\ -2 {input.non_host_r2} \\ --input-profile {output.condense} \\ --output-tsv {output.read_fraction} \\ --output-per-taxon-read-fractions {params.read_fraction_taxa} "],["assembly-binning.html", "Chapter 3 Assembly &amp; Binning 3.1 Metagenomic assembly 3.2 Assembly mapping 3.3 Ensemble binning 3.4 Bin refinement", " Chapter 3 Assembly &amp; Binning Contents here 3.1 Metagenomic assembly megahit \\ -t {threads} \\ --verbose \\ --min-contig-len 1500 \\ -1 {input.r1} -2 {input.r2} \\ -f \\ -o {config[workdir]}/{wildcards.PRB}_{wildcards.EHI}_assembly 3.2 Assembly mapping # Map reads to assembly using Bowtie2 bowtie2 \\ --time \\ --threads {threads} \\ -x {input.contigs} \\ -1 {input.r1} \\ -2 {input.r2} \\ | samtools sort -@ {threads} -o {output} coverm genome \\ -b {input.bam} \\ --genome-fasta-files {input.contigs} \\ -m relative_abundance \\ -t {threads} \\ --min-covered-fraction 0 \\ &gt; {output.coverm} #Run coverm for the eukaryotic assessment pipeline coverm genome \\ -s - \\ -b {input.bam} \\ -m relative_abundance count mean covered_fraction \\ -t {threads} \\ --min-covered-fraction 0 \\ &gt; {output.euk} 3.3 Ensemble binning 3.3.1 Maxbin # summarise contig depths jgi_summarize_bam_contig_depths \\ --outputDepth {params.outdir}/mb2_master_depth.txt \\ --noIntraDepthVariance \\ {input.bam} #calculate total numper of columns A=($(head -n 1 {params.outdir}/mb2_master_depth.txt)) N=${{#A[*]}} # split the contig depth file into multiple files if [ -f {params.outdir}/mb2_abund_list.txt ]; then rm {params.outdir}/mb2_abund_list.txt; fi for i in $(seq 4 $N); do sample=$(head -n 1 {params.outdir}/mb2_master_depth.txt | cut -f $i) grep -v totalAvgDepth {params.outdir}/mb2_master_depth.txt | cut -f 1,$i &gt; {params.outdir}/mb2_${{sample%.*}}.txt if [[ {params.outdir} == /* ]]; then echo {params.outdir}/mb2_${{sample%.*}}.txt &gt;&gt; {params.outdir}/mb2_abund_list.txt else echo {params.outdir}/mb2_${{sample%.*}}.txt &gt;&gt; {params.outdir}/mb2_abund_list.txt fi done # Run maxbin2 run_MaxBin.pl \\ -contig {input.contigs} \\ -markerset 107 \\ -thread {threads} \\ -min_contig_length 1500 \\ -out {params.outdir}/maxbin2_out/bin \\ -abund_list {params.outdir}/mb2_abund_list.txt mkdir -p {params.outdir}/maxbin2_bins for i in {params.outdir}/maxbin2_out/*.fasta; do mv $i {params.outdir}/maxbin2_bins/$(basename ${{i/.fasta/.fa}}); done 3.3.2 Metabat # summarise contig depths jgi_summarize_bam_contig_depths --outputDepth {params.outdir}/metabat_depth.txt {input.bam} # Run metabat2 metabat2 \\ -i {input.contigs} \\ -a {params.outdir}/metabat_depth.txt \\ -o {params.outdir}/metabat2_bins/bin \\ -m 1500 \\ -t {threads} \\ --unbinned 3.3.3 CONCOCT cut_up_fasta.py {input.contigs} -c 10000 --merge_last -b {params.outdir}/assembly_10K.bed -o 0 &gt; {params.outdir}/assembly_10K.fa concoct_coverage_table.py {params.outdir}/assembly_10K.bed {input.bam} &gt; {params.outdir}/concoct_depth.txt concoct \\ -l 1500 \\ -t {threads} \\ --coverage_file {params.outdir}/concoct_depth.txt \\ --composition_file {params.outdir}/assembly_10K.fa \\ -b {params.outdir} merge_cutup_clustering.py {params.outdir}/clustering_gt1500.csv &gt; {params.outdir}/clustering_gt1500_merged.csv mkdir -p {params.outdir}/concoct_bins python {config[codedir]}/scripts/metawrap_split_concoct_bins.py {params.outdir}/clustering_gt1500_merged.csv {input.contigs} {params.outdir}/concoct_bins 3.4 Bin refinement # setup checkm2 db path (in case of first run) checkm2 database --setdblocation {config[checkmdb]} bash {config[codedir]}/scripts/metawrap_refinement.sh \\ -t {threads} \\ -o {params.outdir} \\ -A {params.binning}/concoct_bins/ \\ -B {params.binning}/maxbin2_bins/ \\ -C {params.binning}/metabat2_bins/ \\ -c 50 \\ -x 10 # Rename output files, and sort metawrap by bin name head -1 {params.outdir}/metawrap_50_10_bins.stats &gt; {params.outdir}/mw_colnames.tsv sed &#39;1d;&#39; {params.outdir}/metawrap_50_10_bins.stats | sort -k1,1 -t$&#39;\\t&#39; &gt; {params.outdir}/mw_sorted.tsv cat {params.outdir}/mw_colnames.tsv {params.outdir}/mw_sorted.tsv &gt; {params.outdir}/mw_sorted_col.tsv mv {params.outdir}/mw_sorted_col.tsv {output.stats} mv {params.outdir}/metawrap_50_10_bins.contigs {output.contigmap} sed -i&#39;&#39; &#39;2,$s/bin/{wildcards.EHA}_bin/g&#39; {output.stats} sed -i&#39;&#39; &#39;s/bin/{wildcards.EHA}_bin/g&#39; {output.contigmap} # Rename metawrap bins to match coassembly group: for bin in {params.outdir}/metawrap_50_10_bins/*.fa; do mv $bin ${{bin/bin./{wildcards.EHA}_bin.}}; done # Compress output bins pigz -p {threads} {params.outdir}/metawrap_50_10_bins/*.fa #Print the number of MAGs to a file for combining with the assembly report mkdir -p {params.stats_dir} ls -l {params.outdir}/metawrap_50_10_bins/*.fa.gz | wc -l &gt; {params.stats_dir}/{wildcards.EHA}_bins.tsv # Reformat MAG headers for CoverM for mag in {params.outdir}/metawrap_50_10_bins/*.fa.gz; do rename.sh \\ in=$mag \\ out={params.outdir}/$(basename ${{mag/.fa.gz/_renamed.fa.gz}}) \\ zl=9 \\ prefix=$(basename ${{mag/.fa.gz/^}}); done rm {params.outdir}/metawrap_50_10_bins/*.fa.gz for mag in {params.outdir}/*.fa.gz; do mv $mag {params.outdir}/metawrap_50_10_bins/$(basename ${{mag/_renamed/}}); done "],["bin-annotation.html", "Chapter 4 Bin annotation 4.1 Taxonomic annotation 4.2 Functional annotation", " Chapter 4 Bin annotation Contents here 4.1 Taxonomic annotation # Run GTDB-tk: gtdbtk classify_wf \\ --genome_dir {params.bins} \\ --extension &quot;gz&quot; \\ --out_dir {params.outdir} \\ --cpus {threads} \\ --skip_ani_screen 4.2 Functional annotation DRAM.py annotate \\ -i {input.mag} \\ -o {params.outdir} \\ --threads {threads} \\ --min_contig_size 1500 DRAM.py distill \\ -i {params.outdir}/annotations.tsv \\ --rrna_path {params.outdir}/rrnas.tsv \\ --trna_path {params.outdir}/trnas.tsv \\ -o {params.distillate} "],["dereplication-and-mapping.html", "Chapter 5 Dereplication and mapping 5.1 MAG dereplication 5.2 Mapping to MAG catalogue", " Chapter 5 Dereplication and mapping Contents here 5.1 MAG dereplication dRep dereplicate \\ {config[workdir]}/drep \\ -p {threads} \\ -comp 50 \\ -sa {config[ani]} \\ -g {config[magdir]}/*.fa.gz \\ --genomeInfo mags_formatted.csv 5.2 Mapping to MAG catalogue # Concatenate MAGs cat {config[workdir]}/drep/dereplicated_genomes/*.fa.gz &gt; {config[workdir]}/mag_catalogue/{config[dmb]}_mags.fasta.gz # Index the coassembly bowtie2-build \\ --large-index \\ --threads {threads} \\ {output.mags} {output.mags} bowtie2 \\ --time \\ --threads {threads} \\ -x {input.contigs} \\ -1 {input.r1} \\ -2 {input.r2} \\ | samtools sort -@ {threads} -o {output} coverm genome \\ -b {input} \\ -s ^ \\ -m count covered_fraction \\ -t {threads} \\ --min-covered-fraction 0 \\ &gt; {output.count_table} #relative abundance for report coverm genome \\ -b {input} \\ -s ^ \\ -m relative_abundance \\ -t {threads} \\ --min-covered-fraction 0 \\ &gt; {output.mapping_rate} "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
