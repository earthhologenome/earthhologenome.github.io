[["index.html", "The Earth Hologenome Initiative Bioinformatics Workflow Chapter 1 Introduction 1.1 Workflow overview 1.2 Snakemake pipeline", " The Earth Hologenome Initiative Bioinformatics Workflow Raphael Eisenhofer1 Antton Alberdi2 2023-11-06 Chapter 1 Introduction The Earth Hologenome Initiative (EHI, www.earthhologenome.org) is a global collaborative endeavour aimed at promoting, facilitating, coordinating, and standardising hologenomic research on wild organisms worldwide. The EHI encompasses projects with diverse study designs and goals around standardised and open access sample collection and preservation, data generation and data management criteria. One of the main objectives of the EHI is to facilitate analysis of animal genomic and microbial metagenomic data. Here, you will find a summary of the bioinformatic pipeline we use for generating analytical data from raw sequencing files. 1.1 Workflow overview The genome-resolved metagenomics pipeline of the EHI aims to reconstruct metagenome-assembled genomes (MAGs) from faecal and other samples, and subsequently annotate and analyse these MAGs for a better understanding of microbial community composition and function. Here is an overview of such a pipeline: Data preprocessing: the pipeline begins by assessing and filtering raw sequencing data to remove low-quality reads, adapters, and contaminants. This step ensures the data’s reliability and quality. Splitting host and non-host data: the metagenomic fraction is separated from the host, by mapping the reads against a reference host genome. Metagenomic assembly: the remaining non-host reads are assembled into contigs or scaffolds using metagenomic assembly software. This step results in a set of contigs representing the genetic material of the microbial community. Binning: the assembled contigs are clustered into metagenome-assembled genomes (MAGs) based on sequence composition, coverage, and other characteristics. MAG Annotation: MAGs are annotated to determine their taxonomic identity and functional potential. Dereplication: MAGs are dereplicated to remove redundancy and retain only unique genomic representatives. This step ensures that each MAG represents a distinct microbial population or genome. Compositional overview: finally, to understand the composition of the gut microbiota in individual samples, the pre-processed reads from each sample are mapped against the dereplicated MAG catalogue. This step quantifies the abundance of each MAG in each sample, allowing for a comprehensive view of the microbial community composition. 1.2 Snakemake pipeline Snakemake is a workflow management system that helps automate the execution of computational workflows. It is designed to handle complex dependencies between the input files, output files, and the software tools used to process the data. Snakemake is based on the Python programming language and provides a simple and intuitive syntax for defining rules and dependencies. Here is a brief overview of how Snakemake works and its basic usage: Define the input and output files: In Snakemake, you define the input and output files for each step in your workflow. This allows Snakemake to determine when a step needs to be executed based on the availability of its inputs and the freshness of its outputs. Write rules: Next, you write rules that describe the software tools and commands needed to process the input files into the output files. A rule consists of a name, input and output files, and a command to run. Create a workflow: Once you have defined the rules, you create a workflow by specifying the order in which the rules should be executed. Snakemake automatically resolves the dependencies between the rules based on the input and output files. Run the workflow: Finally, you run the workflow using the snakemake command. Snakemake analyzes the input and output files and executes the rules in the correct order to generate the desired output files. University of Copenhagen, raphael.eisenhofer@sund.ku.dk↩︎ University of Copenhagen, antton.alberdi@sund.ku.dk↩︎ "],["data-preprocessing.html", "Chapter 2 Data preprocessing 2.1 Quality-filtering 2.2 Splitting host and non-host data 2.3 Metagenomic complexity assessment 2.4 Prokaryotic fraction assessment", " Chapter 2 Data preprocessing The first step of the bioinformatic pipeline is to pre-process the raw sequencing data to prepare them for downstream analyses. 2.1 Quality-filtering Raw sequencing data require an initial preprocessing to get rid off low-quality nucleotides and reads, as well as any remains of sequencing adaptors that can mess around in the downstream analyses. An efficient way to do so is to use the software fastp, which can perform all above-mentioned operations in a single go and directly on compressed files. fastp \\ --in1 {input.r1i} --in2 {input.r2i} \\ --out1 {output.r1o} --out2 {output.r2o} \\ --trim_poly_g \\ --trim_poly_x \\ --low_complexity_filter \\ --n_base_limit 5 \\ --qualified_quality_phred 20 \\ --length_required 60 \\ --thread {threads} \\ --html {output.fastp_html} \\ --json {output.fastp_json} \\ --adapter_sequence {params.adapter1} \\ --adapter_sequence_r2 {params.adapter2} 2.2 Splitting host and non-host data Depending on the sample type employed for data generation, sequencing data might contain only host reads, only microbial reads, or a mixture of both. For example, blood sampled from an animal is expected to only contain host DNA/RNA reads (unless an infection is ongoing), while DNA extracted from a microbial culture is only expected to contain microbial DNA/RNA reads (unless human contamination has happened). In contrast, intestinal content samples, faecal samples, leave samples or root samples can contain both host and microbial nucleic acids. Index host genome In order to map metagenomic reads to a reference host genome, it is necessary to index the genome. An index is a data structure that allows for efficient searching of the reference genome by breaking it down into smaller, more manageable pieces. Without an index, aligning reads to a reference genome would be prohibitively slow, especially for large genomes. Bowtie2 is a popular software tool for aligning reads to a reference genome, and it requires an index of the reference genome before alignment can be performed. Bowtie2 uses an index based on the Burrows-Wheeler transform (BWT) algorithm, which enables it to efficiently align reads to the reference genome. Here are the basic command to create a Bowtie2 index for a reference genome: bowtie2-build \\ --large-index \\ --threads {threads} \\ {input.genome} {output.index} Host mapping The next step is to map the reads against the reference genome, followed by a split between reads that have been mapped (in the example below are retained in a BAM/SAM file) and the reads that were not mapped (in the example below outputed to fastq files). The mapped reads can be used for performing population genomic analyses, while the unmapped reads can be used for metagenomic analyses. # Map reads to catted reference using Bowtie2 bowtie2 \\ --time \\ --threads {threads} \\ -x {input.catted_ref} \\ -1 {input.r1i} \\ -2 {input.r2i} \\ | samtools view -b -@ {threads} - | samtools sort -@ {threads} -o {output.all_bam} - &amp;&amp; # Extract non-host reads (note we&#39;re not compressing for nonpareil) samtools view -b -f12 -@ {threads} {output.all_bam} \\ | samtools fastq -@ {threads} -1 {output.non_host_r1} -2 {output.non_host_r2} - &amp;&amp; # Send host reads to BAM samtools view -b -F12 -@ {threads} {output.all_bam} \\ | samtools sort -@ {threads} -o {output.host_bam} - # Get % duplicates from host BAM picard -Xmx{resources.mem_gb}g MarkDuplicates \\ -I {output.host_bam} \\ -O {output.markdup_bam} \\ -M {output.markdup} 2.3 Metagenomic complexity assessment nonpareil \\ -s {input.non_host_r1} \\ -f fastq \\ -T kmer \\ -t {threads} \\ -b {wildcards.sample} #Script to extract nonpareil values of interest Rscript {config[codedir]}/scripts/nonpareil_table.R {output.npo} {output.npstats} 2.4 Prokaryotic fraction assessment #Run singlem pipe singlem pipe \\ -1 {input.non_host_r1} \\ -2 {input.non_host_r2} \\ --otu-table {params.pipe_uncompressed} \\ --taxonomic-profile {output.condense} \\ --threads {threads} #Run singlem read_fraction singlem read_fraction \\ -1 {input.non_host_r1} \\ -2 {input.non_host_r2} \\ --input-profile {output.condense} \\ --output-tsv {output.read_fraction} \\ --output-per-taxon-read-fractions {params.read_fraction_taxa} "],["assembly-binning.html", "Chapter 3 Assembly &amp; Binning 3.1 Metagenomic assembly 3.2 Assembly mapping 3.3 Ensemble binning 3.4 Bin refinement", " Chapter 3 Assembly &amp; Binning The next step is to assemble pre-processed reads into contigs, and to cluster these contigs into bins to reconstruct draft bacterial genomes known as metagenome-assembled genomes or MAGs. 3.1 Metagenomic assembly Metagenomic assembly is a computational process used to reconstruct the genomes of microorganisms present in complex environmental samples, such as faecal samples. Unlike traditional genome sequencing, where a single organism’s genome is targeted, metagenomics focuses on the genetic material of all microorganisms present in a given sample, allowing for the simultaneous study of entire microbial communities. Two of the most popular metagenome assemblers are Megahit and MetaSpades. Metaspades is considered superior in terms of assembly quality, yet memory requirements are much larger than those of Megahit. Thus, one of the most relevant criteria to choose the assembler to be employed is the balance between amount of data and available memory. Another minor, yet relevant difference between both assemblers is that Megahit allows removing contings below a certain size, while MetaSpades needs to be piped with another software (e.g. bbmap) to get rid off barely informative yet often abundant short contigs. Due to the large computational requirements in the EHI, the default metagenomic assembler we use is Megahit. megahit \\ -t {threads} \\ --verbose \\ --min-contig-len 1500 \\ -1 {input.r1} -2 {input.r2} \\ -f \\ -o {config[workdir]}/{wildcards.PRB}_{wildcards.EHI}_assembly The metagenome assemblies can have very different properties depending on the amount of data used for the assembly, the complexity of the microbial community, and other biological and technical aspects. It is therefore convenient to obtain some general statistics of the assemblies to decide whether they look meaningful to continue with downstream analyses. This can be easily done using the software Quast. quast \\ -o {output.report} \\ --threads {threads} \\ {input.assembly} 3.2 Assembly mapping In order to group contigs into MAGs, binners (software that group contigs into bins) require contig coverage information to decide which contig belong to which bin. This type of data is generated by mapping pre-processed metagenomic reads agains the metagenomic assembly. Fist of all, the assembly needs to be indexed. bowtie2-build \\ --large-index \\ --threads {threads} \\ {input.contigs} {input.contigs} Once indexed, the metagenomics reads are mapped against the assembly. # Map reads to assembly using Bowtie2 bowtie2 \\ --time \\ --threads {threads} \\ -x {input.contigs} \\ -1 {input.r1} \\ -2 {input.r2} \\ | samtools sort -@ {threads} -o {output} Using coverM, we can generate the assembly coverage data required for downstream binning steps. coverm genome \\ -b {input.bam} \\ --genome-fasta-files {input.contigs} \\ -m relative_abundance \\ -t {threads} \\ --min-covered-fraction 0 \\ &gt; {output.coverm} #Run coverm for the eukaryotic assessment pipeline coverm genome \\ -s - \\ -b {input.bam} \\ -m relative_abundance count mean covered_fraction \\ -t {threads} \\ --min-covered-fraction 0 \\ &gt; {output.euk} 3.3 Ensemble binning Metagenomic binning is the bioinformatic process that attempts to group metagenomic sequences by their organism of origin. In practice, what binning does is to cluster contigs of a metagenomic assembly into putative bacterial genomes. In the last decade over a dozen of binning algorithms have been released, each relying on different structural and mathematical properties of the input data. Two of the most relevant structural properties to group contigs into bins are oligonucleotide composition of contigs and present of universally conserved genes in contigs. The performance of the binning algorithms is largely dependent on the specific properties of each sample. A software that performs very well with a given sample can be easily outcompeted by another one in the next sample. In consequence, many researchers opt for ensemble approaches whereby assemblies are binned using multiple algorithms, followed by a refinement step that merges all generated information to yield consensus bins. 3.3.1 Maxbin MaxBin2 is a metagenome binning software designed to partition assembled metagenomic contigs into individual genome bins based on sequence composition and abundance. It utilises a combination of tetranucleotide frequency analysis and abundance information obtained from read mapping to identify distinct genomic bins representing different microorganisms within a metagenomic sample. MaxBin2 is known for its efficiency and accuracy in binning metagenomic contigs, making it a valuable tool for characterising microbial communities in various environments. # summarise contig depths jgi_summarize_bam_contig_depths \\ --outputDepth {params.outdir}/mb2_master_depth.txt \\ --noIntraDepthVariance \\ {input.bam} #calculate total numper of columns A=($(head -n 1 {params.outdir}/mb2_master_depth.txt)) N=${{#A[*]}} # split the contig depth file into multiple files if [ -f {params.outdir}/mb2_abund_list.txt ]; then rm {params.outdir}/mb2_abund_list.txt; fi for i in $(seq 4 $N); do sample=$(head -n 1 {params.outdir}/mb2_master_depth.txt | cut -f $i) grep -v totalAvgDepth {params.outdir}/mb2_master_depth.txt | cut -f 1,$i &gt; {params.outdir}/mb2_${{sample%.*}}.txt if [[ {params.outdir} == /* ]]; then echo {params.outdir}/mb2_${{sample%.*}}.txt &gt;&gt; {params.outdir}/mb2_abund_list.txt else echo {params.outdir}/mb2_${{sample%.*}}.txt &gt;&gt; {params.outdir}/mb2_abund_list.txt fi done # Run maxbin2 run_MaxBin.pl \\ -contig {input.contigs} \\ -markerset 107 \\ -thread {threads} \\ -min_contig_length 1500 \\ -out {params.outdir}/maxbin2_out/bin \\ -abund_list {params.outdir}/mb2_abund_list.txt mkdir -p {params.outdir}/maxbin2_bins for i in {params.outdir}/maxbin2_out/*.fasta; do mv $i {params.outdir}/maxbin2_bins/$(basename ${{i/.fasta/.fa}}); done 3.3.2 Metabat MetaBAT2 is another metagenomic binning tool that specialises in clustering metagenomic contigs into genome bins by considering both sequence composition and differential coverage across samples. It employs an innovative algorithm that uses a likelihood-based approach to estimate the probability of contigs belonging to the same genome, allowing for the identification of distinct bins for individual microorganisms. MetaBAT2 is well-regarded for its ability to handle complex microbial communities and is widely used in metagenomic research for its binning accuracy. # summarise contig depths jgi_summarize_bam_contig_depths --outputDepth {params.outdir}/metabat_depth.txt {input.bam} # Run metabat2 metabat2 \\ -i {input.contigs} \\ -a {params.outdir}/metabat_depth.txt \\ -o {params.outdir}/metabat2_bins/bin \\ -m 1500 \\ -t {threads} \\ --unbinned 3.3.3 CONCOCT CONCOCT is a metagenomic binning software that utilises differential coverage and sequence composition to cluster contigs into genome bins. It employs a two-dimensional binning strategy, considering both coverage across multiple samples and sequence composition, allowing for robust binning of metagenomic data. One notable feature of CONCOCT is its capacity to separate closely related strains, which can be challenging for other binning tools. It is particularly valuable for exploring the finer-scale diversity within microbial communities. cut_up_fasta.py {input.contigs} -c 10000 --merge_last -b {params.outdir}/assembly_10K.bed -o 0 &gt; {params.outdir}/assembly_10K.fa concoct_coverage_table.py {params.outdir}/assembly_10K.bed {input.bam} &gt; {params.outdir}/concoct_depth.txt concoct \\ -l 1500 \\ -t {threads} \\ --coverage_file {params.outdir}/concoct_depth.txt \\ --composition_file {params.outdir}/assembly_10K.fa \\ -b {params.outdir} merge_cutup_clustering.py {params.outdir}/clustering_gt1500.csv &gt; {params.outdir}/clustering_gt1500_merged.csv mkdir -p {params.outdir}/concoct_bins python {config[codedir]}/scripts/metawrap_split_concoct_bins.py {params.outdir}/clustering_gt1500_merged.csv {input.contigs} {params.outdir}/concoct_bins 3.4 Bin refinement # setup checkm2 db path (in case of first run) checkm2 database --setdblocation {config[checkmdb]} bash {config[codedir]}/scripts/metawrap_refinement.sh \\ -t {threads} \\ -o {params.outdir} \\ -A {params.binning}/concoct_bins/ \\ -B {params.binning}/maxbin2_bins/ \\ -C {params.binning}/metabat2_bins/ \\ -c 50 \\ -x 10 # Rename output files, and sort metawrap by bin name head -1 {params.outdir}/metawrap_50_10_bins.stats &gt; {params.outdir}/mw_colnames.tsv sed &#39;1d;&#39; {params.outdir}/metawrap_50_10_bins.stats | sort -k1,1 -t$&#39;\\t&#39; &gt; {params.outdir}/mw_sorted.tsv cat {params.outdir}/mw_colnames.tsv {params.outdir}/mw_sorted.tsv &gt; {params.outdir}/mw_sorted_col.tsv mv {params.outdir}/mw_sorted_col.tsv {output.stats} mv {params.outdir}/metawrap_50_10_bins.contigs {output.contigmap} sed -i&#39;&#39; &#39;2,$s/bin/{wildcards.EHA}_bin/g&#39; {output.stats} sed -i&#39;&#39; &#39;s/bin/{wildcards.EHA}_bin/g&#39; {output.contigmap} # Rename metawrap bins to match coassembly group: for bin in {params.outdir}/metawrap_50_10_bins/*.fa; do mv $bin ${{bin/bin./{wildcards.EHA}_bin.}}; done # Compress output bins pigz -p {threads} {params.outdir}/metawrap_50_10_bins/*.fa #Print the number of MAGs to a file for combining with the assembly report mkdir -p {params.stats_dir} ls -l {params.outdir}/metawrap_50_10_bins/*.fa.gz | wc -l &gt; {params.stats_dir}/{wildcards.EHA}_bins.tsv # Reformat MAG headers for CoverM for mag in {params.outdir}/metawrap_50_10_bins/*.fa.gz; do rename.sh \\ in=$mag \\ out={params.outdir}/$(basename ${{mag/.fa.gz/_renamed.fa.gz}}) \\ zl=9 \\ prefix=$(basename ${{mag/.fa.gz/^}}); done rm {params.outdir}/metawrap_50_10_bins/*.fa.gz for mag in {params.outdir}/*.fa.gz; do mv $mag {params.outdir}/metawrap_50_10_bins/$(basename ${{mag/_renamed/}}); done "],["bin-annotation.html", "Chapter 4 Bin annotation 4.1 Taxonomic annotation 4.2 Functional annotation", " Chapter 4 Bin annotation Once the bins of draft bacterial genomes have been generated, one can annotated them both taxonomically as well as functionally, to obtain biological insights into the characterised microbial community. 4.1 Taxonomic annotation Although not necessary for conducting most of the downstream analyses, taxonomic annotation of MAGs is an important step to provide context, improve comparability and facilitate result interpretation in holo-omic studies. MAGs can be taxonomically annotated using different algorithms and reference databases, but the Genome Taxonomy Database (GTDB) and associated taxonomic classification toolkit (GTDB-Tk) have become the preferred option for many researchers. # Run GTDB-tk: gtdbtk classify_wf \\ --genome_dir {params.bins} \\ --extension &quot;gz&quot; \\ --out_dir {params.outdir} \\ --cpus {threads} \\ --skip_ani_screen 4.2 Functional annotation Functional annotation refers to the process of identifying putative functions of genes present in MAGs based on information available in reference databases. The first step is to predict genes in the MAGs (unless these are available from the assembly), followed by functional annotation by matching the protein sequences predicted from the genes with reference databases. Currently, multiple tools exist that perform all these procedures in a single pipeline. For instance, DRAM performs the annotation using Pfam, KEGG, UniProt, CAZY and MEROPS databases. These functional annotations can be used for performing functional gene enrichment analyses, distilling them into genome-inferred functional traits, and many other downstrean operations covered in the EHI analysis workflow. DRAM.py annotate \\ -i {input.mag} \\ -o {params.outdir} \\ --threads {threads} \\ --min_contig_size 1500 DRAM.py distill \\ -i {params.outdir}/annotations.tsv \\ --rrna_path {params.outdir}/rrnas.tsv \\ --trna_path {params.outdir}/trnas.tsv \\ -o {params.distillate} "],["dereplication-and-mapping.html", "Chapter 5 Dereplication and mapping 5.1 MAG dereplication 5.2 Mapping to MAG catalogue", " Chapter 5 Dereplication and mapping The next step of the pipeline is to dereplicate bins into a MAG catalogue, and to map the pre-processed sequencing reads to the MAG catalogue to obtain read-count data per sample and MAG. 5.1 MAG dereplication Dereplication is the reduction of a set of MAGs based on high sequence similarity between them. Although this step is neither essential nor meaningful in certain cases (e.g., when studying straing-level variation or pangenomes), in most cases it contributes to overcome issues such as excessive computational demands, inflated diversity or unspecific read mapping. If the catalogue of MAGs used to map sequencing reads to contains many similar genomes, read mapping results in multiple high-quality alignments. Depending on the software used and parameters chosen, this leads to sequencing reads either being randomly distributed across the redundant genomes or being reported at all redundant locations. This can bias quantitative estimations of relative representation of each MAG in a given metagenomic sample. Dereplication is based on pairwise comparisons of average nucleotide identity (ANI) between MAGs. This implies that the number of comparisons scales quadratically with an increasing amount of MAGs, which requires for efficient strategies to perform dereplication in a cost-efficient way. A popular tool used for dereplicating MAGs is dRep, which combines the fast yet innacurate algorithm MASH with the slow but accurate gANI computation to yield a fast and accurate estimation of ANIs between MAGs. An optimal threshold that balances between retaining genome diversity while minimising cross-mapping issues has been found to be 98% ANI. dRep dereplicate \\ {config[workdir]}/drep \\ -p {threads} \\ -comp 50 \\ -sa {config[ani]} \\ -g {config[magdir]}/*.fa.gz \\ --genomeInfo mags_formatted.csv 5.2 Mapping to MAG catalogue When the objective of a genome-resolved metagenomic analysis is to reconstruct and analyse a microbiome, researchers usually require relative abundance information to measure how abundant or rare each bacteria was in the analysed sample. In order to achieve this, it is necessary to map the reads of each sample back to the MAG catalogue and retrieve mapping statistics. The procedure is identical to that explained in the assembly read-mapping section, yet using the MAG catalogue as a reference database rather than the metagenomic assembly. This procedure usually happens in two steps. In the first step, reads are mapped to the MAG catalogue to generate BAM or CRAM mapping files. In the second step, these mapping files are used to extract quantitative read-abundance information in the form of a table in which the amount of reads mapped to each MAG in each sample is displayed. First, all MAGs need to be concatenated into a single file, which will become the reference MAG catalogue or database. # Concatenate MAGs cat {config[workdir]}/drep/dereplicated_genomes/*.fa.gz &gt; {config[workdir]}/mag_catalogue/{config[dmb]}_mags.fasta.gz The MAG catalogue needs to be indexed before the mapping. # Index the coassembly bowtie2-build \\ --large-index \\ --threads {threads} \\ {output.mags} {output.mags} Then, the following step needs to be iterated for each sample, yielding a BAM mapping file for each sample. bowtie2 \\ --time \\ --threads {threads} \\ -x {input.contigs} \\ -1 {input.r1} \\ -2 {input.r2} \\ | samtools sort -@ {threads} -o {output} Finally, CoverM can be used to extract the required stats, such as relative abundance and covered fraction per MAG per sample. #Relative abundance coverm genome \\ -b {input} \\ -s ^ \\ -m relative_abundance \\ -t {threads} \\ --min-covered-fraction 0 \\ &gt; {output.mapping_rate} #MAG coverage coverm genome \\ -b {input} \\ -s ^ \\ -m count covered_fraction \\ -t {threads} \\ --min-covered-fraction 0 \\ &gt; {output.count_table} "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
